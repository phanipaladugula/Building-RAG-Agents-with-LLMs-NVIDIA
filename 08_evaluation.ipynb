{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [Assessment]:** RAG Evaluation</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the last notebook of the course! In the previous notebook, you integrated a vector store solution into a RAG pipeline! In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learn how to integrate the techniques from prior notebooks to numerically approximate the goodness of your RAG pipeline.\n",
    "\n",
    "- **Final Exercice**: ***By working through this notebook in the Course Environment,* you will be able to submit the coding component of the course!**\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- As you go along, remember what our metrics actually represent. Should our pipeline pass these objectives? Is our judge LLM sufficient for evaluating the pipeline? Does a particular metric even matter for our use case?\n",
    "- If we left the vectorstore-as-a-memory component in our chain, do you think it would still pass the evaluation? Additionally, is the evaluation useful for assessing vectorstore-as-a-memory performance? \n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f59a40-2062-45a7-a2b0-9da530fa581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Setting up the environment ---\n",
      "\n",
      "--- 2. Creating a new document index ---\n",
      "Loading documents from Arxiv...\n",
      "MuPDF error: syntax error: could not parse color space (80 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (389 0 R)\n",
      "\n",
      "Splitting documents...\n",
      "Creating and saving new FAISS index...\n",
      "New index created successfully.\n",
      "\n",
      "--- 3. Building the improved RAG chain ---\n",
      "RAG chain built successfully.\n",
      "\n",
      "--- 4. Generating synthetic Q&A pairs ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">: Question: What is the primary challenge in using large language models for domain-specific applications,</span>\n",
       "<span style=\"font-weight: bold\">particularly in the medical field?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m: Question: What is the primary challenge in using large language models for domain-specific applications,\u001b[0m\n",
       "\u001b[1mparticularly in the medical field?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Answer: The primary challenge is the insufficiency of knowledge, as large language models (LLMs) do not </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">possess sufficient knowledge in specific domains, such as the medical field.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: Answer: The primary challenge is the insufficiency of knowledge, as large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m do not \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpossess sufficient knowledge in specific domains, such as the medical field.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- On iteration 2, the LLM failed to generate a valid Q&A pair. Skipping. ---\n",
      "--- On iteration 3, the LLM failed to generate a valid Q&A pair. Skipping. ---\n",
      "\n",
      "Synthetic Q&A generation complete.\n",
      "\n",
      "--- 5. Answering questions with your RAG chain ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">: Question: What is the primary challenge in using large language models for domain-specific applications,</span>\n",
       "<span style=\"font-weight: bold\">particularly in the medical field?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m: Question: What is the primary challenge in using large language models for domain-specific applications,\u001b[0m\n",
       "\u001b[1mparticularly in the medical field?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: **Direct Answer:** The primary challenge in using large language models for domain-specific </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">applications, particularly in the medical field, is insufficient knowledge.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">**Detailed Explanation:** Recent advancements in large language models (LLMs) have demonstrated impressive </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">capabilities in general-purpose content creation. However, their proficiency in domain-specific applications, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">particularly in the medical field, is notably constrained by insufficient knowledge. This limitation is highlighted</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in the context of using LLMs for medical applications, where the lack of domain-specific knowledge hinders their </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance. The primary challenge lies in the fact that LLMs are not equipped with sufficient knowledge to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">accurately understand and respond to medical-related queries.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To address this challenge, researchers have proposed two main approaches for knowledge infusion: continual </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pre-training on domain-specific corpora and retrieval-augmented methods, which involve integrating external </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge sources. These approaches aim to enhance the domain-specific performance of LLMs by providing them with </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">relevant knowledge and information. However, the effectiveness of these approaches is yet to be fully explored, and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">further research is needed to overcome the challenge of insufficient knowledge in LLMs for domain-specific </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">applications.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">**Citations:**</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Bao et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Han et al. (2023b)</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Zhang et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: **Direct Answer:** The primary challenge in using large language models for domain-specific \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapplications, particularly in the medical field, is insufficient knowledge.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m**Detailed Explanation:** Recent advancements in large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m have demonstrated impressive \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcapabilities in general-purpose content creation. However, their proficiency in domain-specific applications, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mparticularly in the medical field, is notably constrained by insufficient knowledge. This limitation is highlighted\u001b[0m\n",
       "\u001b[1;38;2;118;185;0min the context of using LLMs for medical applications, where the lack of domain-specific knowledge hinders their \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mperformance. The primary challenge lies in the fact that LLMs are not equipped with sufficient knowledge to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0maccurately understand and respond to medical-related queries.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo address this challenge, researchers have proposed two main approaches for knowledge infusion: continual \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpre-training on domain-specific corpora and retrieval-augmented methods, which involve integrating external \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge sources. These approaches aim to enhance the domain-specific performance of LLMs by providing them with \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrelevant knowledge and information. However, the effectiveness of these approaches is yet to be fully explored, and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfurther research is needed to overcome the challenge of insufficient knowledge in LLMs for domain-specific \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapplications.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m**Citations:**\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0m* Bao et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m* Han et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m2023b\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m* Zhang et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG answers generated.\n",
      "\n",
      "--- 6. Evaluating RAG answers ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">Evaluation for QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mEvaluation for QA Pair \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Score: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Justification: The second answer is considered better than the first answer for several reasons. Firstly, it </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">provides a direct and concise answer to the question. Secondly, it offers a more detailed explanation of the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">challenge, including the context and the proposed solutions. The explanation is supported by citations, which adds </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">credibility to the answer. Additionally, the second answer does not introduce any inconsistencies with the first </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">answer; instead, it expands on the idea presented in the first answer, making it a more comprehensive and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">informative response.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mScore: \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mJustification: The second answer is considered better than the first answer for several reasons. Firstly, it \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprovides a direct and concise answer to the question. Secondly, it offers a more detailed explanation of the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mchallenge, including the context and the proposed solutions. The explanation is supported by citations, which adds \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcredibility to the answer. Additionally, the second answer does not introduce any inconsistencies with the first \u001b[0m\n",
       "\u001b[1;38;2;118;185;0manswer; instead, it expands on the idea presented in the first answer, making it a more comprehensive and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minformative response.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete.\n",
      "\n",
      "--- 7. Calculating final score ---\n",
      "\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">  Final Preference Score: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m  Final Preference Score: \u001b[0m\u001b[1;36m1.00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Congratulations! You have a passing score for this notebook exercise.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;38;2;118;185;0mCongratulations! You have a passing score for this notebook exercise.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# FINAL COMBINED SCRIPT FOR NOTEBOOK 08\n",
    "# This cell handles everything: setup, index creation, RAG chain building,\n",
    "# Q&A generation, evaluation, and final score calculation.\n",
    "# ======================================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from operator import itemgetter\n",
    "from functools import partial\n",
    "\n",
    "# --- Part 1: Environment Setup ---\n",
    "print(\"--- 1. Setting up the environment ---\")\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "\n",
    "# Setup for pretty printing\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "# --- Part 2: Create Index and Define Helpers ---\n",
    "print(\"\\n--- 2. Creating a new document index ---\")\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\")\n",
    "print(\"Loading documents from Arxiv...\")\n",
    "docs = ArxivLoader(query=\"Retrieval-Augmented Generation\", load_max_docs=3).load()\n",
    "print(\"Splitting documents...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(\"Creating and saving new FAISS index...\")\n",
    "if os.path.exists(\"docstore_index\"):\n",
    "    shutil.rmtree(\"docstore_index\")\n",
    "docstore = FAISS.from_documents(chunks, embedder)\n",
    "docstore.save_local(\"docstore_index\")\n",
    "print(\"New index created successfully.\")\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "# --- Part 3: Build the Improved RAG Chain ---\n",
    "print(\"\\n--- 3. Building the improved RAG chain ---\")\n",
    "llm = ChatNVIDIA(model='meta/llama-3.1-70b-instruct') | StrOutputParser()\n",
    "\n",
    "# Replace your old chat_prompt with this one\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are an expert research assistant. Your task is to answer the user's question based ONLY on the provided context.\"\n",
    "     \"\\n\\n**Your answer MUST be structured as follows:**\"\n",
    "     \"\\n1. **Direct Answer:** Start with a single, concise sentence that directly answers the question.\"\n",
    "     \"\\n2. **Detailed Explanation:** After the direct answer, provide a comprehensive, multi-paragraph explanation, synthesizing the information from the documents.\"\n",
    "     \"\\n3. **Citations:** Cite only the sources you directly used from the context to formulate your answer.\"\n",
    "     \"\\n\\nContext:\\n{context}\"), \n",
    "    ('user', 'Question: {input}')\n",
    "])\n",
    "\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "context_getter = itemgetter('input') | docstore.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5}) | long_reorder | docs2str\n",
    "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
    "generator_chain = chat_prompt | llm\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "print(\"RAG chain built successfully.\")\n",
    "\n",
    "# --- Part 4: Generate Synthetic Q&A Pairs (Step 3) ---\n",
    "print(\"\\n--- 4. Generating synthetic Q&A pairs ---\")\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
    "    )\n",
    "    usr_msg = (f\"Document1: {format_chunk(doc1)}\\n\\nDocument2: {format_chunk(doc2)}\")\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    \n",
    "    parts = qa_pair.split('\\n\\n')\n",
    "    if len(parts) < 2:\n",
    "        print(f\"--- On iteration {i+1}, the LLM failed to generate a valid Q&A pair. Skipping. ---\")\n",
    "        continue\n",
    "\n",
    "    synth_questions.append(parts[0])\n",
    "    synth_answers.append(parts[1])\n",
    "    pprint2(f\"\\nQA Pair {i+1}: {parts[0]}\")\n",
    "    pprint(f\"Answer: {parts[1]}\")\n",
    "print(\"\\nSynthetic Q&A generation complete.\")\n",
    "\n",
    "# --- Part 5: Answer Synthetic Questions with RAG (Step 4) ---\n",
    "print(\"\\n--- 5. Answering questions with your RAG chain ---\")\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    rag_answer = rag_chain.invoke(q)\n",
    "    rag_answers.append(rag_answer)\n",
    "    pprint2(f\"\\nQA Pair {i+1}: {q}\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\")\n",
    "print(\"\\nRAG answers generated.\")\n",
    "\n",
    "# --- Part 6: Evaluate Answers with LLM-as-a-Judge (Step 5) ---\n",
    "print(\"\\n--- 6. Evaluating RAG answers ---\")\n",
    "eval_instruction = (\n",
    "    \"Evaluate the following Question-Answer pair for human preference and consistency.\"\n",
    "    \"\\nAssume the first answer is a ground truth answer and has to be correct.\"\n",
    "    \"\\nAssume the second answer may or may not be true.\"\n",
    "    \"\\n[0] The second answer lies, does not answer the question, or is inferior to the first answer.\"\n",
    "    \"\\n[1] The second answer is better than the first and does not introduce any inconsistencies.\"\n",
    "    \"\\n\\nOutput Format:\\n[Score] Justification\"\n",
    ")\n",
    "eval_prompt = ChatPromptTemplate.from_messages([('system', eval_instruction), ('user', '{input}')])\n",
    "eval_scores = []\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    usr_msg = f\"Question: {q}\\n\\nAnswer 1: {a_synth}\\n\\n Answer 2: {a_rag}\"\n",
    "    score = (eval_prompt | llm).invoke({'input': usr_msg})\n",
    "    eval_scores.append(score)\n",
    "    pprint2(f\"\\nEvaluation for QA Pair {i+1}:\")\n",
    "    pprint(score)\n",
    "print(\"\\nEvaluation complete.\")\n",
    "\n",
    "# --- Part 7: Calculate and Print Final Score ---\n",
    "print(\"\\n--- 7. Calculating final score ---\")\n",
    "if not eval_scores:\n",
    "    final_score = 0.0\n",
    "else:\n",
    "    final_score = sum((\"[1]\" in score) for score in eval_scores) / len(eval_scores)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "pprint(f\"  Final Preference Score: {final_score:.2f}\")\n",
    "print(\"=\"*30)\n",
    "if final_score > 0.60:\n",
    "    pprint(\"\\nCongratulations! You have a passing score for this notebook exercise.\")\n",
    "else:\n",
    "    print(\"\\nThe score is not yet passing (> 0.60). Consider further tuning the prompt or retriever.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Pre-Release Evaluation\n",
    "\n",
    "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
    "\n",
    "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
    "\n",
    "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
    "\n",
    "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
    "\n",
    "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
    "\n",
    "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
    "\n",
    "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
    "\n",
    "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
    "\n",
    "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
    "\n",
    "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** LLM-as-a-Judge Formulation\n",
    "\n",
    "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
    "\n",
    "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
    "\n",
    "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
    "\n",
    "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
    "\n",
    "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
    "\n",
    "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
    "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
    "- [**LangChain Evaluators**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
    "\n",
    "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f4e86-0541-49fb-850b-f31a4302847a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3: [Assessment Prep]** Pairwise Evaluator\n",
    "\n",
    "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/). \n",
    "\n",
    "**To prepare for our RAG chain evaluation, we will need to:**\n",
    "\n",
    "- Pull in our document index (the one we saved in the previous notebook).\n",
    "- Recreate our RAG pipeline of choice.\n",
    "\n",
    "**We will specifically be implementing a judge formulation with the following steps:**\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "**The chain should be a simple but powerful process that tests for the following objective:**\n",
    "\n",
    "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This will be the system used for the final evaluation!** To see how this system is integrated into the autograder, please check out the implementation in [`frontend/server_app.py`](frontend/server_app.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1:** Pull In Your Document Retrieval Index\n",
    "\n",
    "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [],
   "source": [
    "# from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# # Use the standard, general-purpose text embedding model. This is the correct model.\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\")\n",
    "\n",
    "# # Unzip the index file\n",
    "# !tar xzvf docstore_index.tgz\n",
    "\n",
    "# # Load the FAISS index using the correct embedder\n",
    "# docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "\n",
    "# docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "# def format_chunk(doc):\n",
    "#     return (\n",
    "#         f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "#         f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "#         f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "#     )\n",
    "\n",
    "# ## This printout just confirms that your store has been retrieved\n",
    "# print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "# pprint(f\"\\nSample Chunk:\\n\\n{format_chunk(docs[len(docs)//2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2: [Exercise]** Pull In Your RAG Chain\n",
    "\n",
    "Now that we have our index, we can recreate the RAG agent from the previous notebook! \n",
    "\n",
    "**Key Modifications:**\n",
    "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [],
   "source": [
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnableLambda, RunnableAssign\n",
    "# from langchain.document_transformers import LongContextReorder\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# from operator import itemgetter\n",
    "\n",
    "# # Define your LLM and helper functions\n",
    "# llm = ChatNVIDIA(model='mixtral_8x7b') | StrOutputParser()\n",
    "\n",
    "# def docs2str(docs, title=\"Document\"):\n",
    "#     out_str = \"\"\n",
    "#     for doc in docs:\n",
    "#         doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "#         if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "#         out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "#     return out_str\n",
    "\n",
    "# chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "#     \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "#     \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "#     \" The following information may be useful for your response: \"\n",
    "#     \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "#     \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"), ('user', '{input}')])\n",
    "\n",
    "# def output_puller(inputs):\n",
    "#     for token in inputs:\n",
    "#         if token.get('output'):\n",
    "#             yield token.get('output')\n",
    "\n",
    "# # Define the RAG chain correctly\n",
    "# long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "# context_getter = itemgetter('input') | docstore.as_retriever() | long_reorder | docs2str\n",
    "# retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
    "# generator_chain = chat_prompt | llm\n",
    "# rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# # Test the chain\n",
    "# for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "#     print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
    "\n",
    "In this section, we can implement the first few part of our evaluation routine:\n",
    "\n",
    "- **Sample the RAG agent document pool to find two document chunks.**\n",
    "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# num_questions = 3\n",
    "# synth_questions = []\n",
    "# synth_answers = []\n",
    "\n",
    "# simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
    "\n",
    "# for i in range(num_questions):\n",
    "#     doc1, doc2 = random.sample(docs, 2)\n",
    "#     sys_msg = (\n",
    "#         \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "#         \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "#         \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "#         \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
    "#     )\n",
    "#     usr_msg = (\n",
    "#         f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "#         f\"Document2: {format_chunk(doc2)}\"\n",
    "#     )\n",
    "\n",
    "#     qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "#     synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "#     synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "#     pprint2(f\"QA Pair {i+1}\")\n",
    "#     pprint2(synth_questions[-1])\n",
    "#     pprint(synth_answers[-1])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 4:** Answer The Synthetic Questions\n",
    "\n",
    "In this section, we can implement the third part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- **Use the RAG agent to generate its own answer.**\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [],
   "source": [
    "# ## TODO: Generate some synthetic answers to the questions above.\n",
    "# rag_answers = []\n",
    "# for i, q in enumerate(synth_questions):\n",
    "#     # Use the rag_chain that was just created to get an answer\n",
    "#     rag_answer = rag_chain.invoke(q)\n",
    "#     rag_answers += [rag_answer]\n",
    "#     pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
    "#     pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 5:** Implement A Human Preference Metric\n",
    "\n",
    "In this section, we can implement the fourth part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [],
   "source": [
    "# ## TODO: Adapt this prompt for whichever LLM you're actually interested in using. \n",
    "# ## If it's llama, maybe system message would be good?\n",
    "# eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION: \n",
    "# Evaluate the following Question-Answer pair for human preference and consistency.\n",
    "# Assume the first answer is a ground truth answer and has to be correct.\n",
    "# Assume the second answer may or may not be true.\n",
    "# [1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
    "# [2] The second answer is better than the first and does not introduce any inconsistencies.\n",
    "\n",
    "# Output Format:\n",
    "# [Score] Justification\n",
    "\n",
    "# {qa_trio}\n",
    "\n",
    "# EVALUATION: \n",
    "# \"\"\")\n",
    "\n",
    "# pref_score = []\n",
    "\n",
    "# trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "# for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "#     pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "#     qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "#     pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
    "#     pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "#     pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "#     pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [],
   "source": [
    "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Advanced Formulations\n",
    "\n",
    "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action. \n",
    "\n",
    "With that being said, this metric was merely a product of us specifying:\n",
    "- **What kind of behavior is important for our pipeline to have?**\n",
    "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
    "\n",
    "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
    "\n",
    "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
    "\n",
    "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
    "\n",
    "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/stable/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine. \n",
    "\n",
    "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
    "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/trajectory/) is a good starting point.\n",
    "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/v0.1/docs/modules/agents/) is a good starting point!\n",
    "\n",
    "<br>\n",
    "\n",
    "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5: [Assessment]** Evaluating For Credit\n",
    "\n",
    "Welcome to the last exercise of the course! Hopefully you've enjoyed the material and are ready to actually get credit for these notebooks! For this part:\n",
    "\n",
    "- **Make sure you're in the course environment**\n",
    "- **Make sure `docstore_index/` has been uploaded to the course environment...**\n",
    "    - **...and contains [at least one Arxiv paper](https://arxiv.org/search/advanced) which has been updated recently.**\n",
    "- **Make sure you don't have some old session of [`09_langserve.ipynb`](09_langserve.ipynb) already occupying the port. Your assessment requires you to implement the new `/retriever` and `/generator` endpoints!!**\n",
    "\n",
    "**Objective:** On launch, [**`frontend/frontend_block.py`**](frontend/frontend_block.py) had several lines of code which trigger the course pass condition. Your objective is to invoke that series of commands by using your pipeline to pass the **Evaluation** check! Recall [`09_langserve.ipynb`](09_langserve.ipynb) and use it as a starting example! As a recommendation, consider duplicating it so that you can keep the original as an authoritative reference. \n",
    "\n",
    "**Once Finished:** While your course environment is still open, please navigate back to your course environment launcher area and click the **\"Assess Task\"** button! After that, you're all done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = 'http://'+window.location.host+':8090';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**Congratulations On Completing The Course**</font>\n",
    "\n",
    "Hopefully this course was not only exciting and challenging, but also adequately prepared you for work on the cutting edge of LLM and RAG system development! Going forward, you should have the skills necessary to tackle industry-level challenges and explore RAG deployment with open-source models and frameworks.\n",
    "\n",
    "**Some NVIDIA-specific releases related to this that you may find interesting include:**\n",
    "- [**NVIDIA NIM**](https://www.nvidia.com/en-us/ai/), which offers microservice spinup routines that can be deployed on local compute.\n",
    "- [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.\n",
    "- [**NVIDIA's Generative AI Examples Repo**](https://github.com/NVIDIA/GenerativeAIExamples), which includes the current canonical microservice example application and will be updated with new resources as new production workflows get released.\n",
    "- [**The Knowledge-Based Chatbot Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief) which discusses additional publicly-accessible details on productionalizing RAG systems.\n",
    "\n",
    "**Additionally, some key topics you may be interested in delving more into include:**\n",
    "- [**LlamaIndex**](https://www.llamaindex.ai/), which has strong components that can augment and occasionally improve upon the LangChain RAG features.\n",
    "- [**LangSmith**](https://docs.smith.langchain.com/), an upcoming agent productionalization service offered by LangChain.\n",
    "- [**Gradio**](https://www.gradio.app/), though touched on in the course, has many more interface options which will be worth investigating. For inspiration, consider checking out [**HuggingFace Spaces**](https://huggingface.co/spaces) for examples.\n",
    "- [**LangGraph**](https://python.langchain.com/docs/langgraph/) is a framework for graph-based LLM orchestration, and is a natural next step forward for those interested in [multi-agent workflows](https://blog.langchain.dev/langgraph-multi-agent-workflows/).\n",
    "- [**DSPy**](https://github.com/stanfordnlp/dspy), a flow engineering framework that allows you to optimize LLM orchestration pipelines based on empirical performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
   "metadata": {
    "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad7644-3086-4697-b5fd-5f599b366839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bee98d-8302-4eee-884e-2e232d8106cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41f497-54f7-4e51-9f60-6e8f75443fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ef57d-a389-44f3-8f1c-e29b522204bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fac0e-c9c0-4628-bed5-c4644a5fb88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93adcb-853c-4694-9d0e-cbb076ed4a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb338260-ae4c-4bec-83f4-a507a5992864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfbb0e7-e672-47d7-80c5-d7cb00106da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a3fc6-06fd-4195-9ce9-37b4f024ebb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcb66b-b17c-4dc9-a497-161ba0c769c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
